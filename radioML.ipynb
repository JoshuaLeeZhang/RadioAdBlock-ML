{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnF\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Audio Chunks\n",
    "\n",
    "The following section processes audio files by loading, concatenating, and splitting them into 10-second chunks based on their labels. It saves the concatenated audio data as .npy files and the chunks as .wav files for further analysis.\n",
    "\n",
    "Original files are found in the RadioLabels folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = glob('RadioLabels/*/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_path(path):\n",
    "    split_path = path.split('\\\\')\n",
    "    label = split_path[1]\n",
    "    filename = split_path[2].split('.')[0]\n",
    "    \n",
    "    return label, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_concatenate(files):\n",
    "    music = []\n",
    "    not_music = []\n",
    "    global_sample_rate = None\n",
    "    \n",
    "    for file in files:\n",
    "        label, filename = split_path(file)\n",
    "        waveform, sample_rate = librosa.load(file)\n",
    "        \n",
    "        if global_sample_rate is None:\n",
    "            global_sample_rate = sample_rate\n",
    "        else:\n",
    "            assert global_sample_rate == sample_rate\n",
    "            \n",
    "        if label == 'Music':\n",
    "            music.extend(waveform)\n",
    "        else:\n",
    "            not_music.extend(waveform)\n",
    "            \n",
    "    # Save music and not_music to files\n",
    "    output_folder = 'concatenated_audio'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    np.save(os.path.join(output_folder, 'music.npy'), np.array(music))\n",
    "    np.save(os.path.join(output_folder, 'not_music.npy'), np.array(not_music))\n",
    "    \n",
    "    return global_sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(audio_data_npy_path, sr, chunk_duration=10):\n",
    "    audio_data = np.load(audio_data_npy_path)  # Load the audio data from the numpy file\n",
    "    label = os.path.splitext(os.path.basename(audio_data_npy_path))[0]\n",
    "    chunk_samples = int(chunk_duration * sr)  # Calculate the number of samples for each 10-second chunk\n",
    "    chunks = []\n",
    "    \n",
    "    # Split audio into chunks of specified duration\n",
    "    for i in range(0, len(audio_data), chunk_samples):\n",
    "        chunk = audio_data[i:i + chunk_samples]\n",
    "        if len(chunk) == chunk_samples:  # Only keep chunks that are exactly 10 seconds\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    # Create a new folder to store the chunks\n",
    "    output_folder = os.path.join('10_second_chunks', f'{label}_audio_chunks')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Save each chunk as a separate file\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk_filename = os.path.join(output_folder, f'{label}_chunk_{idx}.wav')\n",
    "        sf.write(chunk_filename, chunk, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and concatenate all audio files\n",
    "sample_rate = load_and_concatenate(audio_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split concatenated audio into 10-second chunks\n",
    "os.makedirs('10_second_chunks', exist_ok=True)\n",
    "\n",
    "split_into_chunks('concatenated_audio\\\\music.npy', sample_rate)\n",
    "split_into_chunks('concatenated_audio\\\\not_music.npy', sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio chunks\n",
    "\n",
    "music_audio_chunk_paths = glob('10_second_chunks/music_audio_chunks/*.wav')\n",
    "not_music_audio_chunk_paths = glob('10_second_chunks/not_music_audio_chunks/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that all audio chunks have the same sample rate of 22050 Hz\n",
    "\n",
    "SAMPLE_RATE = 22050\n",
    "\n",
    "def verify_same_sample_rate(audio_paths):\n",
    "    for audio_path in audio_paths:\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        assert sr == SAMPLE_RATE\n",
    "\n",
    "verify_same_sample_rate(music_audio_chunk_paths)\n",
    "verify_same_sample_rate(not_music_audio_chunk_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the MFCC Transformer \n",
    "\n",
    "mfcc_params = {\n",
    "    'sample_rate': SAMPLE_RATE,\n",
    "    'n_mfcc': 20,\n",
    "    'melkwargs': {\n",
    "        'n_fft': 2048,\n",
    "        'n_mels': 128,\n",
    "        'hop_length': 512,\n",
    "        'mel_scale': 'htk'\n",
    "    }\n",
    "}\n",
    "\n",
    "mfcc_transform = T.MFCC(**mfcc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_store_mfccs(audio_chunk_paths, label):\n",
    "    os.makedirs(f'mfcc/{label}_mfcc', exist_ok=True)\n",
    "    \n",
    "    for path in audio_chunk_paths:\n",
    "        waveform = torchaudio.load(path)[0]\n",
    "        mfcc = mfcc_transform(waveform)\n",
    "        output_path = f'mfcc/{label}_mfcc/mfcc_{os.path.basename(path).split('.')[0]}.npy'        \n",
    "        np.save(output_path, mfcc.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('mfcc', exist_ok=True)\n",
    "\n",
    "compute_and_store_mfccs(music_audio_chunk_paths, 'music')   \n",
    "compute_and_store_mfccs(not_music_audio_chunk_paths, 'not_music')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NpyDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for label, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            files = [file for file in os.listdir(class_dir) if file.endswith('.npy')]\n",
    "            for file in files:\n",
    "                self.file_paths.append(os.path.join(class_dir, file))\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        tensor = torch.from_numpy(np.load(file_path))  # Load .npy file and convert to tensor\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if len(tensor.shape) == 2:  # If the tensor is 2D, add a channel dimension\n",
    "            tensor = tensor.unsqueeze(0)  # Adds a channel dimension at the beginning\n",
    "        \n",
    "        return tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "dataset = NpyDataset(root_dir='mfcc')\n",
    "\n",
    "# Set split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Calculate split lengths\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "val_size = int(val_ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for each subset\n",
    "batch_size = 32 \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_height is the number of frequency bins in the MFCC\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, input_height, input_channels=1, conv_channels=16, hidden_size=128, num_classes=2):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, conv_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(conv_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(conv_channels, conv_channels * 2, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(conv_channels * 2)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.05)\n",
    "\n",
    "        # Calculate output dimensions after convolution and pooling\n",
    "        conv_output_height = input_height // 4  # Two pooling layers reduce height\n",
    "\n",
    "        # Calculate LSTM input size\n",
    "        self.lstm_input_size = (conv_channels * 2) * conv_output_height\n",
    "\n",
    "        # Initialize LSTM and Fully Connected Layers\n",
    "        self.lstm = nn.LSTM(input_size=self.lstm_input_size, hidden_size=hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nnF.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nnF.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        # Reshape for LSTM      \n",
    "        x = x.permute(0, 3, 1, 2)  # (batch, time_steps, channels, freq)\n",
    "        batch_size, time_steps, channels, freq = x.shape\n",
    "        \n",
    "        # Calculate lstm_input_size based on observed shapes\n",
    "        x = x.contiguous().view(batch_size, time_steps, -1)\n",
    "\n",
    "        # LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        # Classification\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "model = CRNN(input_height=20)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()  # for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)  # or another optimizer\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0  # Track total training loss for this epoch\n",
    "    for tensors, labels in train_loader:\n",
    "        tensors, labels = tensors.to(device), labels.to(device)  # Move to GPU if available\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(tensors)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()  # Accumulate training loss\n",
    "\n",
    "    # Calculate average training loss for this epoch\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout, etc.)\n",
    "    val_loss = 0  # Track total validation loss for this epoch\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for val_tensors, val_labels in val_loader:\n",
    "            val_tensors, val_labels = val_tensors.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_tensors)\n",
    "            val_loss += criterion(val_outputs, val_labels).item()  # Accumulate validation loss\n",
    "\n",
    "    # Calculate average validation loss for this epoch\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Print training and validation loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'models/model_4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshz\\AppData\\Local\\Temp\\ipykernel_18812\\905399977.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('models/model_4.pth')\n"
     ]
    }
   ],
   "source": [
    "import exportsd\n",
    "\n",
    "f = open(\"model_weights.dat\", \"wb\")\n",
    "exportsd.save_state_dict(model.to(\"cpu\").state_dict(), f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No gradients needed for testing\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m test_tensors, test_labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_loader\u001b[49m:\n\u001b[0;32m      6\u001b[0m         test_tensors, test_labels \u001b[38;5;241m=\u001b[39m test_tensors\u001b[38;5;241m.\u001b[39mto(device), test_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# Forward pass on the test set\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing phase after training is complete\n",
    "model.eval()  # Set model to evaluation mode\n",
    "test_loss = 0\n",
    "with torch.no_grad():  # No gradients needed for testing\n",
    "    for test_tensors, test_labels in test_loader:\n",
    "        test_tensors, test_labels = test_tensors.to(device), test_labels.to(device)\n",
    "        \n",
    "        # Forward pass on the test set\n",
    "        test_outputs = model(test_tensors)\n",
    "        test_loss += criterion(test_outputs, test_labels).item()\n",
    "\n",
    "# Calculate average test loss\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"Testing Loss: {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import Precision, Recall, F1Score\n",
    "\n",
    "num_classes = 2  # Update this to the actual number of classes in your dataset\n",
    "precision = Precision(task='multiclass', num_classes=num_classes, average='macro').to(device)\n",
    "recall = Recall(task='multiclass', num_classes=num_classes, average='macro').to(device)\n",
    "f1 = F1Score(task='multiclass', num_classes=num_classes, average='macro').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Assume test_loader is a DataLoader\n",
    "for inputs, targets in test_loader:\n",
    "    # Move data to the appropriate device\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "    # Use raw logits/probabilities directly\n",
    "    precision.update(outputs, targets)\n",
    "    recall.update(outputs, targets)\n",
    "    f1.update(outputs, targets)\n",
    "\n",
    "# Compute metrics\n",
    "precision_value = precision.compute()\n",
    "recall_value = recall.compute()\n",
    "f1_value = f1.compute()\n",
    "\n",
    "print(f\"Precision: {precision_value:.4f}\")\n",
    "print(f\"Recall: {recall_value:.4f}\")\n",
    "print(f\"F1 Score: {f1_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = CRNN(input_height=20, input_channels=1, conv_channels=16, hidden_size=128, num_classes=2)\n",
    "model.to(device)\n",
    "summary(model, input_size=(1, 1, 20, 431))  # Adjust for batch and input dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a dummy input with the correct shape for your model\n",
    "dummy_input = torch.randn(1, 1, 20, 431).to(device)  # Adjust dimensions as needed for your model\n",
    "\n",
    "# Forward pass to get the output and visualize the graph\n",
    "output = model(dummy_input)\n",
    "graph = make_dot(output, params=dict(model.named_parameters()))\n",
    "\n",
    "# Save the graph to a file\n",
    "graph.format = 'png'  # Specify the format (e.g., png, pdf, svg)\n",
    "graph.attr(dpi='400')\n",
    "graph.render('crnn_model_graph')  # This will save 'crnn_model_graph.png' in the current directory\n",
    "\n",
    "# Display the graph using Matplotlib\n",
    "img = plt.imread('crnn_model_graph.png')\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load('models/model_4.pth')\n",
    "\n",
    "batch = next(iter(train_loader))  # Get the first batch from the DataLoader\n",
    "\n",
    "# If the DataLoader returns inputs and labels\n",
    "inputs, labels = batch  # Unpack the batch\n",
    "dummy_input = inputs.to(device)     # Use the inputs as the dummy_input\n",
    "\n",
    "torch.onnx.export(loaded_model, dummy_input, \"model_4.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save for TorchSharp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exportsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshz\\AppData\\Local\\Temp\\ipykernel_20268\\962322818.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('models/model_4.pth')\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('models/model_4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_weights.dat\", \"wb\") as f:\n",
    "    exportsd.save_state_dict(model.to(\"cpu\").state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.05, inplace=False)\n",
       "  (lstm): LSTM(160, 128, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0355, -0.0725]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1, 1, 20, 431))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
